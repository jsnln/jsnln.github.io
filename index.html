<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Siyou Lin</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Siyou Lin</name>
              </p>
              <p style="text-align:center">
                Email: <a href="mailto:linsy21@mails.tsinghua.edu.cn">linsy21@mails.tsinghua.edu.cn</a>
              </p>
              <p>I am currently a PhD student at the Department of Automation, Tsinghua University, China, supervised by <a href="http://liuyebin.com/">Prof. Yebin Liu</a>. My current research is focused on data-driven 3D human avatars and cloth animation. My research involves different subfields in CV & CG, including geometry processing, rendering, animation and deep learning. I am also actively investigating generative models.                
              </p>
              <p>Prior to this, I received my Bachelor's degree in Mathematics from the Department of Mathematics, Tsinghua University. I was also a member of the <a href="https://www.tsinghua.edu.cn/en/Admissions/Undergraduate/Tsinghua_Xuetang_Talents_Program.htm">Tsinghua Xuetang Mathematics Talent Program</a>. During my undergraduate studies, I worked closely with <a href="https://shizqi.github.io/">Prof. Zuoqiang Shi</a> from Yau Mathematical Sciences Center, Tsinghua University and <a href="https://binwangthss.github.io/">Prof. Bin Wang</a> from School of Software, Tsinghua University, on the problem of point cloud surface reconstruction.</p>
              <p>Other than my educational experiences, I also worked on monocular 3D depth estimation in a remote internship at the GRASP lab, UPenn, supervised by <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> from 2020 to 2021. I also spent time at Meta Zurich as a research scientist intern working on realistic dynamic clothing animation in 2024.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <!-- siggraphasia2024_wnnc -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='teasers/siggraphasia2024_wnnc.jpg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="wnnc/index.html">
                <papertitle>Fast and Globally Consistent Normal Orientation based on the Winding Number Normal Consistency</papertitle>
              </a>
              <br>
              <strong>Siyou Lin</strong>, <a href="https://shizqi.github.io/">Zuoqiang Shi</a>, <a href="http://liuyebin.com/">Yebin Liu</a>
              <br>
              <em><strong>SIGGRAPH Asia 2024 (Journal)</strong></em>
              <br>
              <a href="wnnc/index.html">Project page</a>
              /
              <a href="https://arxiv.org/abs/2405.16634">Paper</a>
              /
              <a href="https://github.com/jsnln/WNNC">Code</a>
              <p></p>
              <p>
                We present Winding Number Normal Consistency (WNNC) and an iterative algorithm that estimates globally consistent normal vectors. Our CUDA-based treecode-accelerated algorithm handles a million of points in half a minute.
              </p>
          </tr>
        </tbody></table>

        <!-- siggraph2024_layga -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='teasers/siggraph2024_layga.jpg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="layga/index.html">
                <papertitle>LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer</papertitle>
              </a>
              <br>
              <strong>Siyou Lin</strong>, <a href="https://lizhe00.github.io/">Zhe Li</a>, <a href="https://suzhaoqi.github.io/">Zhaoqi Su</a>, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>, <a href="http://liuyebin.com/">Yebin Liu</a>
              <br>
              <em><strong>SIGGRAPH 2024 (Conference)</strong></em>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3641519.3657501">ACM DL</a>
              /
              <a href="layga/index.html">Project page</a>
              /
              <a href="https://arxiv.org/abs/2405.07319">Paper</a>
              <p></p>
              <p>
                We present LayGA, a layered Gaussian avatar model based on Animatable Gaussians, with improved geometry and the ability of animatable clothing transfer.
              </p>
          </tr>
        </tbody></table>

        <!-- iccv2023_intrinsic -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='teasers/iccv2023_intrinsic.jpg' width=180; height="auto">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="iccv2023_intrinsic/index.html">
                  <papertitle>Leveraging Intrinsic Properties for Non-Rigid Garment Alignment</papertitle>
                </a>
                <br>
                <strong>Siyou Lin</strong>, Boyao Zhou, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>, <a href="http://liuyebin.com/">Yebin Liu</a>
                <br>
                <em><strong>ICCV 2023</strong></em>
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Leveraging_Intrinsic_Properties_for_Non-Rigid_Garment_Alignment_ICCV_2023_paper.html">CVF page</a>
                /
                <a href="iccv2023_intrinsic/index.html">Project page</a>
                /
                <a href="https://arxiv.org/abs/2308.09519">Paper</a>
                /
                <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Lin_Leveraging_Intrinsic_Properties_ICCV_2023_supplemental.zip">Supp</a>
                /
                <a href="https://github.com/jsnln/IntrinsicGarmAlign">Code</a>
                /
                <a href="iccv2023_intrinsic/assets/iccv4349_poster.pdf">Poster</a>
                <p></p>
                <p>
                  We leverage <strong>intrinsic manifold properties and neural deformation fields</strong> and propose a coarse-to-fine two-stage method for non-rigid garment alignment, achieving wrinkle-level and texture-level alignment.
                </p>
            </tr>
        </tbody></table>

        <!-- iccv2023_caphy -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='teasers/iccv2023_caphy.jpg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://suzhaoqi.github.io/projects/CaPhy/">
                <papertitle>CaPhy: Capturing Physical Properties for Animatable Human Avatars</papertitle>
              </a>
              <br>
              <a href="https://suzhaoqi.github.io/">Zhaoqi Su</a>, <a href="https://huliangxiao.github.io/">Liangxiao Hu</a>, <strong>Siyou Lin</strong>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>, <a href="http://homepage.hit.edu.cn/zhangshengping">Shengping Zhang</a>, <a href="https://justusthies.github.io/" rel="external nofollow noopener" target="_blank">Justus Thies</a>, <a href="http://www.liuyebin.com/">Yebin Liu</a>
              <br>
              <em><strong>ICCV 2023</strong></em>
              <br>
              <a href="https://suzhaoqi.github.io/projects/CaPhy/">Project page</a>
              /
              <a href="https://suzhaoqi.github.io/assets/pdf/CaPhy.pdf">Paper</a>
              <p></p>
              <p>
                We present CaPhy, a novel method combining 3D-supervised training with unsupervised physics-based losses for reconstructing animatable human avatars. Caphy allows estimating the physical properties of garments and producing realistic dynamic clothing.
              </p>
          </tr>
        </tbody></table>

        <!-- cvpr2023_closet -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='teasers/cvpr2023_closet.jpg' width=180; height="auto">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.liuyebin.com/closet/">
              <papertitle>CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition</papertitle>
            </a>
            <br>
            <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>, <strong>Siyou Lin</strong>, <a href="https://dsaurus.github.io/saurus">Ruizhi Shao</a>, Yuxiang Zhang, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, Han Huang, Yandong Guo, <a href="http://liuyebin.com/">Yebin Liu</a>
            <br>
            <em><strong>CVPR 2023</strong></em>
            <br>
            <a href="https://www.liuyebin.com/closet">Project page</a>
            /
            <a href="https://arxiv.org/abs/2304.03167">Paper</a>
            /
            <a href="https://github.com/HongwenZhang/THuman-CloSET">Dataset</a>
            <p></p>
            <p>
              We present CloSET, a point-based human avatar modeling method that allows loose clothing modeling by explicit template decomposition. We also release a real-world capture dataset of clothed humans in various poses.
            </p>
        </tr>
        </tbody></table>

        <!-- eccv2022_fite -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='teasers/eccv2022_fite.jpg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="fite/index.html">
                <papertitle>Learning Implicit Templates for Point-Based Clothed Human Modeling</papertitle>
              </a>
              <br>
              <strong>Siyou Lin</strong>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, <a href="https://dsaurus.github.io/saurus">Ruizhi Shao</a>, <a href="http://liuyebin.com/">Yebin Liu</a>
              <br>
              <em><strong>ECCV 2022</strong></em>
              <br>
              <a href="fite/index.html">Project page</a>
              /
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3747_ECCV_2022_paper.php">ECVA page</a>
              /
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630211.pdf">Paper</a>
              /
              <a href="fite/assets/fite_supp.pdf">Supp</a>
              /
              <a href="https://github.com/jsnln/fite">Code</a>
              /
              <a href="fite/assets/3747-poster.pdf">Poster</a>
              <p></p>
              <p>
                We propose a <strong>First-Implicit-Then-Explicit (FITE)</strong> framework that combines the merits of both implicit and explicit representations for modeling human avatars in clothing.
              </p>
          </tr>
        </tbody></table>

        <!-- tog2022_pgr -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='teasers/tog2022_pgr.jpg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="tog2022_pgr/index.html">
                <papertitle>Surface Reconstruction from Point Clouds without Normals by Parametrizing the Gauss Formula</papertitle>
              </a>*
              <br>
              <strong>Siyou Lin</strong>, Dong Xiao, <a href="https://shizqi.github.io/">Zuoqiang Shi</a>, <a href="https://binwangthss.github.io/">Bin Wang</a>
              <br>
              <em><strong>ACM Trans. Graph. 2022. Presented at SIGGRAPH 2023</strong></em>
              <br>
              <a href="tog2022_pgr/index.html">Project page</a>
              /
              <a href="https://dl.acm.org/doi/pdf/10.1145/3554730">Paper</a>
              /
              <a href="https://github.com/jsnln/ParametricGaussRecon">Code</a>
              /
              <a href="tog2022_pgr/assets/pgr_ff_compresed.mp4">Fast-forward video</a>
              /
              <a href="tog2022_pgr/assets/pgr-pre.pdf">Slides (SIGGRAPH 2023)</a>
              <p></p>
              <p>
                We present <strong>Parametric Gauss Reconstruction (PGR)</strong>, a method that reconstructs surfaces from point clouds without normals by parametrizing the Gauss formula.
              </p>
              <p>
                *Work done while I was an undergraduate at the Department of Mathematical Sciences, Tsinghua University.
              </p>

          </tr>
					
        </tbody></table>

        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This template is adapted from <a href="https://github.com/jonbarron/jonbarron_website">this website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
